[33m06c16de3d3[m[33m ([m[1;36mHEAD[m[33m -> [m[1;32mislemyakoubi/issue37333[m[33m, [m[1;31morigin/myContributions[m[33m, [m[1;31morigin/main[m[33m, [m[1;31morigin/islemyakoubi/issue37333[m[33m, [m[1;31morigin/deep-copy-groundingdino[m[33m, [m[1;31morigin/HEAD[m[33m, [m[1;32mmain[m[33m, [m[1;32mdeep-copy-groundingdino[m[33m)[m Enable RUF013 to enforce optional typing (#37266)
[33mf6664ee713[m Add ALL_ATTENTION_FUNCTIONS compatibility for Pixtral model (#37960)
[33m015b6dfbf8[m Fix `pad` image transform for batched inputs (#37544)
[33m5c47d08b0d[m Add Swin2SR ImageProcessorFast (#37169)
[33m17742bd9c8[m 🔴 [VLM] Add base model without head  (#37033)
[33m3fa8d9c20e[m [CSM] tiny fix on generation (#38001)
[33m798f948e88[m Add CSM model (#36719)
[33mc8607a17cb[m Add a check to import_utils.py to allow for use of faiss_gpu installation (#37997)
[33mfb1e3a4daa[m remove duplicate code (#37991)
[33m8a9441d26d[m [chat template] separate jinja logic from tokenizers  (#37602)
[33m038f8fc159[m make aya vision 5 integration tests pass on xpu (#37990)
[33ma9384f849a[m [offload] respect `max_memory` argument when factoring in unused reserved memory (#37982)
[33m0b037fd425[m Fix Qwen models export with torch 2.7 (#37985)
[33m3c0796aaea[m [Fast Processor] BEiT (#37005)
[33mebbe9b12dd[m Fix donut backtracking (#37788)
[33m06c4d05fe6[m Enable granite speech 3.3 tests (#37560)
[33m031ef8802c[m fix FSDP + torch.compile bug when saving pretrained model  (#37725)
[33m5534b80b7f[m enable xpu in test_trainer (#37774)
[33m7db5d5b9ea[m Fix typo (#37964)
[33maf2866a8b1[m [speech2text] fix init of sinusoidal embeddings  (#37931)
[33m274e79b326[m Fix typos (#37978)
[33m057ae00504[m Small typo lines 47 and 199 perf_infer_gpu_one.md (#37938)
[33mcc68070d41[m fix docs serving typos. (#37936)
[33mb1375177fc[m add job links to new model failure report (#37973)
[33macded47fe7[m [llava] one pixel is missing from padding when length is odd (#37819)
[33m9981214d32[m [tests] Smaller model in slow cache tests (#37922)
[33mff5ef95db7[m add xpu memory check  (#37969)
[33m7cc78804ba[m 🚨🚨🚨 Fix forward of Dinov2ForImageClassification for models with registers (#37836)
[33m471958b620[m Add GraniteMoeHybrid support for 4.0 (#37658)
[33mfe29b8c487[m [Ready to Merge][HFQuantizer] Squelch pydantic warnings (#37726)
[33m46c0e1ff80[m Fix incorrect type annotation in get_auxiliary_logits (#37955)
[33md80f53fa50[m [generate] Fix `vocab_size` access for multimodal models (#37937)
[33m7819911b0c[m Use T4 single GPU runner with more CPU RAM (#37961)
[33m3b067a15dd[m [core] reuse unused reserved cuda memory when loading models (#37920)
[33mafbc293e2b[m More fault tolerant notification service (#37924)
[33m36ca58bf4f[m [D-FINE] Update names (#37957)
[33m2932f318a2[m [docs] logits docstring (#37929)
[33mfa3c3f9cab[m Break weight tying when quantizing input embedding (#37905)
[33m8a0a508f2b[m Aligning modling code for GPT2 to work with vLLM (fallback) (#36934)
[33me94a4807df[m Add usage example for DINOv2 (#37398)
[33md20aa68193[m 🌐 [i18n-KO] Translated `gpu_selection.md` to Korean (#36757)
[33mee25d57ed1[m Improve performance of `load_state_dict` (#37902)
[33m410aa01901[m [chat] clean code and add base help (#37892)
[33m5b573bebb9[m Fix typos in strings and comments (#37910)
[33mc80f65265b[m 🚨 rm already deprecated pad_to_max_length arg (#37617)
[33m7a3e208892[m fixed gemma3 collection path pointing to llama 2 collection. (#37899)
[33m86777b5e2f[m Support `AOPerModuleConfig` and `include_embedding` (#37802)
[33mc3aeaa8060[m Enhance documentation to explain chat-based few-shot prompting (#37828)
[33m36e2e33bbe[m Fix Qwen3 tp plan with FP8 (#37871)
[33m8e8025b384[m [tests] reset logs in `torch.compile` test (#37894)
[33m1b222903c3[m [tests] Test all cache implementations (#37873)
[33m2c1155519f[m Support FlaxPreTrainedModel to load model checkpoint from local subfolder safetensors (#37732)
[33m5b223bbc8c[m update comment in image_processing_base.py to reference image_process… (#37864)
[33m0dffcb0967[m Fix: reassign in qwen3 moe model (#37848)
[33m6c5d374d56[m uniformize kwargs for VisionTextDualEncoder (#34563)
[33m4fc976779e[m Fix qwen2-vl-docs. (#37879)
[33m4eb6acc896[m make sure lr is not a tensor (#37881)
[33m7be92f9a94[m fix error for _register_pytree_node in torch2.1.0 and  fix bf16 assertion in xpu and npu (#37839)
[33m455c3a33b0[m update Clean_up_tokenization_spaces typos. (#37865)
[33md538293f62[m Transformers cli clean command (#37657)
[33m63cd4c76f3[m Llama Guard updates (#37872)
[33m34f26e2c3e[m enable internvl UTs on XPU (#37779)
[33ma57274466f[m Allow override inputs to export recipe (#37508)
[33m481de7204c[m Skip is_flaky tests in the CI (#37723)
[33m5f8d17268c[m Update modeling_llama4.py (#37841)
[33m50f8caaa48[m 🌐 [i18n-KO] Translated `electra.md` to Korean (#36763)
[33m91f3e9422f[m Add Intel Gaudi doc (#37855)
[33mc34afa5957[m Processor chat template: pass custom kwargs (#37852)
[33m66ad8b2db0[m docs: Details for ambigious channel dimension assignment (#37600)
[33m096f25ae1f[m Fix Bitnet tokenizer in pipeline (#37861)
[33mda7ae467c4[m Fix cache get item return type hints (#37847)
[33maa6b79db43[m Fix check of unecessary packages (issue #37626) (#37825)
[33m517367fe9a[m Revert change that breaks on Torch 2.1 (#37531)
[33m755b0fa2fe[m [tests] reorganize cache tests and clean memory between tests (#37684)
[33m3a1acc36ed[m [tests] fix flaky pattern in `test_generate_continue_from_past_key_values` (#37724)
[33m4abeb50f6e[m Add D-FINE Model into Transformers (#36261)
[33m4602059aae[m [modular] Fix the prefix-based renaming if the old and new model share a common name suffix (#37829)
[33ma847d4aa6b[m Fast image processor for VitMatte added and bug in slow version fixed (#37616)
[33m65e940208c[m Samhq model addition  (#35147)
[33m9c5b1319d0[m [config] revert #37603 (#37821)
[33m9e730689c3[m change XLA deprecated api (#37741)
[33m2933894985[m Fix error of HPU TP (#37782)
[33mda4ff2a5f5[m Add Optional to remaining types (#37808)
[33m1a9188a54e[m FIX: Faulty PEFT tests (#37757)
[33mb262680af4[m Add Bitnet model (#37742)
[33m82862ce443[m [RT-DETR] Improve docs (#37814)
[33m97e57b2545[m Fix: Correct tensor shape comment in Mamba modeling (#37801)
[33m33493542aa[m [doc] fix the code examples in qwen doc (#37803)
[33md5fa7d2d19[m Fix typos in strings and comments (#37799)
[33mf466603963[m Define warmup allocator for torchao quantization (#37764)
[33ma41b6d9b5c[m Fix the fsdp config cannot work issue. (#37549)
[33m816b37010c[m Gemma3 is Torch Exportable (#37728)
[33m397a5ede33[m Fix error message in `hub.py` (#37796)
[33m6ce675ee81[m fix performance issue in convert_ids_to_tokens (#37773)
[33m57c620bf8a[m chore: update SigLIP2 model card (#37624)
[33meb4afdd1fb[m [i18n-KO] Translated `keypoint_detection.md` to Korean (#36649)
[33m555693fbfa[m fix mpt test of different outputs from cuda (#37691)
[33m0cfbf9c95b[m Force torch>=2.6 with torch.load to avoid vulnerability issue (#37785)
[33meefc86aa31[m Fix tensor parallel with non-floating dtypes (#37790)
[33m214062201e[m Fix typos in strings and comments (#37784)
[33mba3bd37253[m Align gpt2 mask preparation to #37612 (#37787)
[33m50d231a806[m unpin pytest<8 (#37768)
[33m79d4bc761d[m [causal mask] fix preparation with multi-gpu (#37612)
[33m7bb619d710[m 🌐 [i18n-KO] Translated `roberta.md` to Korean (#37069)
[33mcfe666919e[m Update model card for Gemma (#37674)
[33mb2d70e9c49[m Fix auto-round hfoption  (#37759)
[33macdbe627e3[m Guard DeepSpeed imports (#37755)
[33maf6d2756d9[m [deps] pin max `torch` version  (#37760)
[33m0302aa1c6e[m Fix typos in comments (#37694)
[33maf000ceb92[m Fix load of rng state for resuming training from checkpoint (#37162)
[33m0af0a5f969[m Fix tied weight loading with TP and loading sub state_dicts (#37758)
[33m3af24f7e27[m Refine parameter type annotations (#37666)
[33m22e3da92b7[m Fix wrong input shapes in doc-string of models (#37729)
[33m4d64c38593[m [generate] fix default autocompile case on gpu (#37756)
[33m43bb4c0456[m Fix qwen2_5 get_rope_index tensor device locations (#37597)
[33mdd2649fa98[m updated hidden_features for FlaxDinov2SwiGLUFFN in Dinov2  (#37747)
[33m8bdd4f2acd[m [generate] skip compilation on cpu offload (#37709)
[33m7c62e69326[m `GPT2Model` StaticCache support (#35761)
[33m9f927c8250[m [cache] fix `HybridCache` init when `device` is passed (#37718)
[33m4fee320926[m Expand quantized data type support for tensor parallelism  (#37719)
[33m0f7940bb3f[m Update `MllamaForConditionalGenerationIntegrationTest` (#37750)
[33m7e6f36cd38[m Skip all `AriaForConditionalGenerationIntegrationTest` on `T4` (#37746)
[33m0327d0f7f2[m [performance_optim] define flash attention mask on NPU device directly (#37698)
[33m14e28bd721[m Correctly raise errors when downloading tokenizer files (#37740)
[33m0ec0495967[m Fix `embeds_to_talker` device in Qwen2.5-Omni (#37739)
[33m72e4844059[m fix: learning_rate logged as tensor causing save issue with deepspeed (#37704)
[33m1cfcbfcab8[m [VLMs] fix flash-attention tests (#37603)
[33m02baa61fab[m Make sure torch_is_available before using torch.distributed (#37693)
[33m864e9636ff[m [tests] fix `test_nemotron_8b_generation_sdpa` (#37665)
[33m9b3bf4a206[m Fix torchao doc examples (#37697)
[33m3ed56bea0f[m Fix inference bugs in Qwen2.5 Omni (#37701)
[33mb7f7aa78a0[m Fix Aria tests (#37444)
[33mb6d65e40b2[m Add Fast Image Processor for MobileNetV1  (#37111)
[33mdea1919be4[m Add Fast Image Processor for PoolFormer (#37182)
[33mb491f128d6[m Add Fast PVT Processor (#37204)
[33m19e9079dc1[m enable 4 test_trainer cases on XPU (#37645)
[33m5cd6b64059[m Process inputs directly in apply_chat_template in image-text-to-text pipeline (#35616)
[33m80ea2c05c2[m [tests, `qwen2_5_omni`] fix flaky tests (#37721)
[33m63c6331387[m Qwen 2.5 Omni: apply video defaults (#37660)
[33m1e9087368c[m [internvl] fix chat template (#37656)
[33m9ec8be56dd[m TransfoXL is deprecated, don't keep it in tested examples! (#37707)
[33mbe9b0e8521[m [CI] add back `sacrebleu` (and document why) (#37700)
[33m1d7d7a942e[m Add maintainers for ROCm/Intel XPU/Ascend NPU (#37678)
[33mcc9a245e6d[m [cleanup] remove `/model_cards` 🧹 🧹  (#37685)
[33mca790303f7[m Pin torch == 2.6 on PR CI docker images for now (#37695)
[33m12f65ee752[m enable cpu offloading for Bark on xpu (#37599)
[33m4f9893cbbc[m fix: remove classmethod from `Qwen2_5OmniConfig.get_text_config` (#37690)
[33m1d9743edc2[m Updated model card for mbart and mbart50 (#37619)
[33mfbfa1dd4db[m 🌐 [i18n-KO] Translated `siglip.md` to Korean (#37145)
[33mece79b0688[m enable blip2 and emu3 cases on XPU (#37662)
[33mca4c114dc4[m Add counters for dataset classes (#37636)
[33md47cdae27e[m [Docs] Move models to appropriate section (#37338)
[33mdbfccd3c92[m typo update in the parameter name (#37655)
[33mde8916dde6[m [docs] only build `en` docs in push CI (#37677)
[33m0f8c34b0a0[m [cleanup] remove old scripts in `/scripts` 🧹 🧹  (#37676)
[33m6673081b21[m enable 6 granite cases on xpu (#37569)
[33m9167461a7d[m enable mllama cases on xpu (#37644)
[33mde182ba269[m Refactor bitsandbytes doc (#37668)
[33mdde9b03e3b[m Fix no_split_modules for Llama4 pretrained models (#37673)
[33m9481e9e9f1[m Fix autoround docs  (#37675)
[33m38c406844e[m Fixing quantization tests (#37650)
[33mb3492ff9f7[m Add AutoRound quantization support (#37393)
[33m9608908639[m Correct warm-up with fp8 (#37670)
[33m6614209b96[m Fix duplicated weights in fp8 quantization (#37667)
[33mdcf6df5b0d[m [qwen-omni] fix training (#37517)
[33m9167fadab9[m Introduce GradientCheckpointingLayer (#37223)
[33m413f9bbf80[m Fixes #37219 : RecurrentGemma crashes for inputs longer than sliding window length (#37613)
[33m964a1b6b7d[m Fix ValueError when eval_do_concat_batches=False with examples (#37621)
[33m85665a4263[m [tests] Stricter generate + compilation test -- no recompilations allowed (#37629)
[33m362fa37da2[m [test] update `test_past_key_values_format` (#37614)
[33m1cd110c6cb[m Add test to ensure unknown exceptions reraising in utils/hub.py::cached_files() (#37651)
[33mc69e23455d[m Support loading Gemma3 QAT GGUF models (#37649)
[33m7eb1107cc2[m Restructure torchao quantization examples (#37592)
[33m006530d285[m [fix gemma] Set default value for output_attentions parameter in Gemma2 and Gemma… (#37633)
[33m31ea547b7a[m [fix] make legacy bnb code work (#37331)
[33m5f791281c3[m Fix Qwen2.5-Omni get_chunked_index chunking functionality (#37631)
[33mfee1190601[m Refactor phi doc (#37583)
[33mb2db54f66b[m Update longformer.md (#37622)
[33m2c60a442f3[m fix link in kv_cache.md (#37652)
[33ma42ba80fa5[m Allow Exclusion of Input IDs from RepetitionPenaltyLogitsProcessor (#37625)
[33m1077603410[m Remove torchvision requirement from AutoImageProcessor (#37457)
[33m1930e750e4[m [kernels] use original forward at compile time (#37604)
[33m6daa3eeba5[m Fix InternVL attention when using qk_norm (38B and 78B) (#37620)
[33m27a25bee4f[m chore: update model card for SigLIP (#37585)
[33me1f379bb09[m Fixing the example in generation strategy doc (#37598)
[33m4f58fc9c82[m Deprecate modeling_utils.py classes (#37298)
[33ma245011252[m Add InternVL (2.5 MPO) (#35968)
[33mb0c6ff5e13[m fix issue that some example with no trainer use accelerator.end_train… (#37435)
[33m6f5014ac31[m fix 2 encoder_decoder issues on XPU (#37572)
[33m2ba6b92a6f[m [VLMs] use only `xxx_token_id` for multimodal tokens (#37573)
[33m4afd3f4820[m Model debugger upgrades (#37391)
[33me5ac23081e[m [Gemma3] compile ✨  (#37447)
[33ma1b82563f1[m enable 6 modeling cases on XPU (#37571)
[33m3cd6627cd7[m enable 6 gemma2 cases on XPU (#37564)
[33m049b75ea72[m Flag SpeechT5 flaky test (#37587)
[33maa17cfb4d5[m [Bugfix] Fix flash-attention func param mismatch and softmax_scale default value mistake on Ascend NPU (#37575)
[33m14b3dbcf3b[m remove _run_third_party_device_tests (#37445)
[33mf974214353[m Fix some GPU OOM after #37553 (#37591)
[33m438324c9cf[m Gaudi: Add the bf16 support for hpu (#37568)
[33mbb2a44ad4b[m Fix Quark quantization config (#37578)
[33m4acf692ace[m Update Phi4 converter (#37594)
[33m40cba20e87[m Ensure positive warm-up size (#37581)
[33m346f1eebbd[m docs: fix typo (#37567)
[33m48dd89cf55[m [phi4] update conversion (#37579)
[33m58e5e976e0[m Small fix on context manager detection (#37562)
[33mc7d3cc67a1[m Fix qwen2audio wanr -> warn (#37559)
[33mdc06e7cecd[m [TimesFM] use the main revison instead of revision for integration test (#37558)
[33m3bc44eaaee[m [qwen-vl] Standardize config (#37268)
[33m4f96081aad[m [chat template] fix security vulnerability (#37523)
[33ma2ef3cf537[m Add Janus model (#36053)
[33m688f4707bf[m All models can be initialized on meta device (#37563)
[33m0a83588c51[m Bridgetower fast image processor (#37373)
[33m4005730044[m Fix Mamba2 Grouped SSD Support in the torch_forward Path (#37533)
[33ma7d2bbaaa8[m Add EfficientNet Image PreProcessor (#37055)
[33m32eca7197a[m [vlm] adjust max length for special tokens (#37342)
[33mc94c59fc47[m Fix pixel attention mask padding in smolvlm (#37497)
[33m5a6de703a7[m Run `test_can_load_with_global_device_set` using a subprocess (#37553)
[33m9a4ce64770[m :red_circle: Update CLIP vision attention to new attention interface (#37498)
[33mdc8227827d[m Fix TimesFm doc issue (#37552)
[33m2f517200c1[m Make Ignored Columns ValueError More Informative (#33299)
[33m0577cae808[m Fix device issue for tapas (with `as_tensor`) (#37551)
[33mb33edf1b9b[m docs(typo): Update ISSUES.md, fix a small typo (#37542)
[33m503541d7ef[m add FlashAttentionKwargs and seq_idx to flat collator (#36456)
[33m9ddcf5fce5[m Update quantization docs (#37439)
[33ma91020aed0[m Add TimesFM Time Series Forecasting Model (#34082)
[33m8669c016d2[m Refactor torchao docs  (#37490)
[33me3d3b54638[m Keep Quark loading through meta device (#37538)
[33m61436a9323[m convert scale and zero to cuda when using HQQ backend (#37425)
[33m7752e7487c[m Fixes hqq by following a new path for bias parameter in pre_quantized models (#37530)
[33m7dafcd0077[m More appropriate cuda warmup in resource-constrained hardware (#37550)
[33m6fd87d1172[m Add Fast Grounding-Dino Processor (#37108)
[33med53809ac5[m enable 6 rt_detr_v2 cases on xpu (#37548)
[33md91858c232[m enable 3 mpt test cases on XPU (#37546)
[33m4541c2cdef[m Fix BitsAndBytesConfig JSON serialization in TrainingArguments (#37520)
[33ma335dc4d6d[m enable `test_offloaded_cache_implementation` on XPU (#37514)
[33m33f6c5a5c8[m enable several cases on XPU (#37516)
[33m5ab7a7c640[m enable 5 cases on XPU (#37507)
[33m3165eb7c28[m Refactor ColPali model documentation (#37309)
[33m33c6fdb2cf[m Update VITS model card (#37335)
[33m4cc6b60654[m Fix broken add-fast-image-processor CLI (#37499)
[33m51f544a4d4[m Add Fast Conditional-DETR Processor (#37071)
[33m4f1dbe8152[m Add Fast Chinese-CLIP Processor (#37012)
[33mc08997c52e[m VDR task guide (#37485)
[33m57da364d8e[m fix and enhance pipeline_webserver.md (#36992)
[33m356b3cd71d[m Fix missing return type for MLCD docs (#37527)
[33m0ad3710d47[m fix: Restore explicit error surfacing for unexpected hub exceptions (#37525)
[33mf6c79f767c[m Add Fast Yolos Processor (#37292)
[33mecaeee66bc[m Llama4: remove redundant transpose of router_logits (#37468)
[33m6f7ea1cf00[m Add MLCD model (#36182)
[33md6ac923ad9[m Change default value of `attn_temperature_tuning` (#37501)
[33mc8e0e603de[m Detect and use device context manager or global device in `from_pretrained` (#37216)
[33m4e63a1747c[m Don't auto-assign reviewers when the author is in HF (#37500)
[33m8ab296501a[m Remove deprecation warning for `num_logits_to_keep` (#37149)
[33m20ceaca228[m Add Fast owlvit Processor (#37164)
[33mcb39f7dd5b[m [qwen-omni] fix processor (#37493)
[33md228f50acc[m Fixing gated repo issues (#37463)
[33ma5dfb98977[m Fix wrong argparse type in modular checker script (#37472)
[33ma53a63c9c2[m Add Fast Mobilenet-V2 Processor (#37113)
[33m4774a39d05[m Add ImageProcessorFast to BiT processor (#37180)
[33me43f168eb3[m Add Fast LeViT Processor (#37154)
[33m1efcfa9ca4[m Fix mask handling for flex attention in llama/gemma2/mistral/qwen2 (#37381)
[33m86064035f0[m [bug] deprecated deta load_cuda_kernel, MultiScaleDeformableAttention (#37443)
[33m7cc9e61a3a[m Add Fast Image Processor for Donut (#37081)
[33m4e53840920[m Detect and fix most `_init_weights()` issues - make it work for composite models (#37070)
[33m1897a02d83[m Add Fast Image Processor for LayoutLMv3 (#37201)
[33m7bff4bdcf6[m Fixed broken links (#37466)
[33me16775d103[m Add Fast Image Processor for LayoutLMv2 (#37203)
[33m49b9a69a36[m Add Fast Image Processor for Flava (#37135)
[33ma5079a2c84[m [ci] fix doc builder (#37489)
[33me7f5724efd[m Add Fast Image Processor for Perceiver (#37176)
[33m4b8c6d4cf8[m Add Qwen2.5-Omni (#36752)
[33mac1df5fccd[m Fix tests failed with gated repos. (#37484)
[33m1ef64710d2[m Remove `fsspec` dependency which isn't directly used by transformers (#37318)
[33m47b9f06aa2[m make test_snowman_image_captioning pass on XPU, by sharing same atol w/ ROCM (#37480)
[33m78cea3e22c[m fix: (llama4) fix no_split_modules to be picked up for fsdpv1 and v2 sharding (#37462)
[33m953196a43d[m Fix typing issues with SigLip2 (#37356)
[33maaf129cdae[m [agents] remove agents 🧹  (#37368)
[33m69e6ddf27f[m Delete hubconf.py (#37455)
[33m623d395aff[m Add Granite Speech Support (#36801)
[33m435f88f1db[m nit: typing use Llama4TextConfig instead of Llama4Config (#37430)
[33m954f31cd81[m Add XPU case to is_torch_bf16_gpu_available (#37132)
[33m28eae8b4bd[m Add weights_only=True to torch.load (#37062)
[33mbf46e44878[m :rotating_light: :rotating_light: Allow saving and loading multiple "raw" chat template files (#36588)
[33m897874748b[m Disable kernels for quantization (#37446)
[33m6a75528cbc[m prevent creating a view/leaf param for low rank optimizers w FSDP (#37379)
[33m6cef03ba66[m [Regression] Fix Quark quantized model loading after refactorization (#37407)
[33ma563999a02[m [processor] clean up mulitmodal tests (#37362)
[33m3c39c07939[m Remove triton mlp kernel, not compiling for some models (#37449)
[33mf797e3d98a[m Fix the test fetcher (#37452)
[33m442d356aa5[m Add moe kernels (#37376)
[33m7e9b57ce62[m Update-kernel-pin (#37448)
[33m54a123f068[m Simplify soft dependencies and update the dummy-creation process (#36827)
[33m931126b929[m Fixes: Corrects file path for CUDA kernels (#37438)
[33mc7064cdba1[m enhance require_deterministic_for_xpu (#37437)
[33m371c44d0ef[m Remove old code for  PyTorch,  Accelerator and tokenizers (#37234)
[33m7ff896c0f2[m [Feat] Support npu in modeling models (#37369)
[33m10907e2846[m Adding to self_comment_ci.yml (#37426)
[33m7d76876498[m (Part 2) feat: allow for tp_size attr for tplizing the model (#37054)
[33mdac443414e[m fix: use mtime by default in Trainer._rotate_checkpoints with automatic fallback (#37260)
[33m6daec12d0b[m Add GGUF support to Gemma3 Text backbone (#37424)
[33m0ea1151222[m Llama Kernel integration (#37092)
[33m9c0c323e12[m Fix require_read_token (#37422)
[33mbde41d69b4[m Correctly drop tokens in SwitchTransformer (#37123)
[33m7ecc5b88c0[m Add image classifier donut & update loss calculation for all swins  (#37224)
[33m5ae9b2cac0[m Quark Quantization gated repo (#37412)
[33md9e76656ae[m Fix new failure reports not including anything other than `tests/models/` (#37415)
[33m1ae8d54b04[m [chat-template] Unify tests and clean up 🧼  (#37275)
[33m10144ff116[m use `rms_norm_eps` for the L2Norm for Llama4 (#37418)
[33maa478567f8[m Allow rocm systems to run these tests (#37278)
[33mae5ce22664[m from_pretrained should handle xpu case (#37382)
[33m4f139f5a50[m Send trainer/fsdp/deepspeed CI job reports to a single channel (#37411)
[33ma2c2fb0108[m update `kernels` to 0.4.3 (#37419)
[33m0ddad2d655[m mark llama4 as not supported with fa2 (#37416)
[33mfbb2054ed5[m Offloaded hybrid cache for Llama4 (#37401)
[33m6d8b0b3378[m Fix Llama4 offset (#37414)
[33mf5865d32a2[m Restrict & Explain tp_plan for FBgemm (#37404)
[33me39c732644[m Handle torch ver in flexattn (#37400)
[33mbc0150bb04[m Add warning when failed to acquire other user's lock at model download (#37395)
[33m9cda4265d6[m handle torch version edge cases (#37399)
[33me032d12e8a[m the fix that did not get in (#37370)
[33mf834ca2c19[m Attention Quantization with FBGemm & TP (#37384)
[33mc5c648dd74[m Fix some failing AWQ tests (#37383)
[33m71b35387fd[m Apply torchfix to replace deprecated functions: `_pytree._register_pytree_node` and `torch.cpu.amp.autocast` (#37372)
[33mad340908e4[m Fix warning message for PEFT models in text-generation pipeline #36783 (#36887)
[33m2527f71a47[m Add "selecting a quantization method" doc (#37159)
[33m7ae0be722e[m update deepspeed docker (#37371)
[33me3eda6d188[m Add glm4 (#37388)
[33m1e6ff5fd55[m fix: llama4 conversion script no_rope_layers (#37359)
[33m6f4058aee3[m Update composition flag usage (#36263)
[33m08e3217baf[m Preserve requires_grad in pre quantized model (#37354)
[33m4d0de5f73a[m :rotating_light: :rotating_light: Setup -> setupclass conversion (#37282)
[33mc15a7adb28[m fix(qwen): fix shape error when using tp (#36947)
[33m121f91d36c[m prune LM Head for USD (#36695)
[33m4321b0648c[m [core] remove `GenerationMixin` inheritance by default in `PreTrainedModel` (#37173)
[33maab0878327[m Skip non-selected experts for mixtral and qwen2_moe (#32429)
[33m35f0f5b5da[m [llama 4] dynamic rope decorator (#37365)
[33m530322ccb6[m Set vision config to None for Gemma 1B conversion (#37366)
[33m8064cd9b4f[m fix deepspeed job (#37284)
[33mcdfb018d03[m A bit of cleaning 🧹🧹 (#37215)
[33m1e6b546ea6[m Use Python 3.9 syntax in tests (#37343)
[33m0fc683d1cd[m convert float for yarn related arguments in rope_scaling (#37139)
[33m2515a5a290[m Expose blip2qformer (#37254)
[33m2da82e432d[m Multiple llama4 fixe (#37353)
[33m794fde7b1c[m Fixing flex attention for torch=2.6.0 (#37285)
[33mb54c2f4689[m more fixes for post-training llama4 (#37329)
[33m754a370bca[m Remove unnecessary attr assignment (#36837)
[33m31a62c2eb8[m Updated Model-card for donut (#37290)
[33mf830105183[m Add bnb to the list of supported quantization methods for LLama4 (#37348)
[33me2b0224d94[m Update Model Card for Jamba (#37152)
[33m6cc109c354[m Improvements in Gemma2 model card (#37076)
[33m8bbcdf5409[m Clean up the compressed-tensors integration (#37349)
[33m3a826a45ca[m Update Model card for GPT2 (#37101)
[33m5e855095a2[m Update falcon mamba card (#37253)
[33m416b5a875d[m Update model-card for DINOv2 (#37104)
[33mf8a16805c5[m updated model card for Mistral (#37156)
[33m48e179857c[m Remove HQQ from caching allocator warmup (#37347)
[33m832cb684a0[m Update translation template (#37294)
[33m22065bd645[m fix derived berts `_init_weights` (#37341)
[33mf789f960c8[m Avoid build crashes when torch.version.xpu doesn't exist and fix Llama4 processor tests (#37346)
[33m12bf24d6ae[m enable 2 llama UT cases on xpu (#37126)
[33me7ad077012[m byebye torch 2.0 (#37277)
[33m99f9f1042f[m Fix torchao usage (#37034)
[33m0fb8d49e88[m Use Python 3.9 syntax in examples (#37279)
[33m08f36771b3[m Fix `init empty weights` without accelerate (#37337)
[33m9db31ea585[m Fix deepspeed with quantization (#37324)
[33mdebfe904c9[m fix llama4 training (#37319)
[33m54538ebee3[m fix flex attn when optional args aren't passed (#37327)
[33md1b92369ca[m v4.52.0.dev0
[33m25b7f27234[m Add llama4 (#37307)
[33maa40fda346[m Hf Xet extra (#37305)
[33me94571580b[m Fix deepspeed loading (part 2) (#37306)
[33m84aa13dd85[m Fix deepspeed loading (#37281)
[33m0ef339ff1b[m Update OpenAI GPT model card (#37255)
[33m46d73910d5[m Updated T5 model card with standardized format (#37261)
[33m579135a2f6[m Updated model card for distilbert (#37157)
[33m8cd57eb731[m mobilebert model card update (#37256)
[33mebe47ce3e9[m Fix: Unexpected Keys, Improve `run_compressed`, Rename Test Folder (#37077)
[33m531e4fcf0e[m Update model card for Depth Anything (#37065)
[33ma4e55fcff8[m Disable delay_optimizer_creation in `Trainer` to support fsdp2 (#37147)
[33m878562b68d[m fix test device spec relative path importing issue (#37190)
[33m8ebc435267[m Fix llava_onevision tests (#37280)
[33mad3d157188[m [RoPE] abstract dynamic RoPE update under a decorator ✨  (#37249)
[33m3d40bda30e[m Hugging Face Hub pin to v0.30.0 for Xet (#37166)
[33macbcb5d07d[m [Tests] flaky `test_constrained_beam_search_generate_dict_output`  (#37276)
[33m4ba0989eab[m Clarify error message to ensure min 28x28 image supplied for Qwen 2.5 VL (#37264)
[33m352ec8ef22[m pin specific `natten` version in docker file  (#37274)
[33medd345b52e[m Fix deprecated PT functions (#37237)
[33mb016de1ae4[m Fix `utils/check_bad_commit.py` (#37272)
[33mf74d7da836[m Introduce modular files for speech models (#35902)
[33md130cd0e16[m update error msg (#37207)
[33m41b9b92b52[m [qwen-vl] fix image processor (#37258)
[33m8dd0a2b89c[m Update model card for electra (#37063)
[33m15ac2b6ac5[m Update Model Card for ModernBERT (#37052)
[33mb552708694[m chore: Update model doc for code_llama (#37115)
[33m2b84831a93[m Update model card for Cohere (#37056)
[33m2d46a08b63[m Purge unused ModelTester code (#37085)
[33m1b29409d89[m feat: updated model card for qwen_2.5_vl (#37099)
[33m8a828a747e[m Add Optional to types (#37163)
[33m3f6af96732[m Adding links to ShieldGemma 2 technical report (#37247)
[33m9a1c1fe7ed[m [CI] green llama tests (#37244)
[33m782d7d945d[m Allow flexible generation params arg when checking pipeline specs (#37211)
[33mafafb84b59[m Add support for fast image processing in image-pretraining example (#37021)
[33m34ccfebf32[m Fix AST parsing when looking for remote code imports (#37245)
[33mf697b3f824[m enable 2 types of case on XPU (#37198)
[33m2099287a59[m [CI] lazy loading external datasets (#37218)
[33ma0803a9555[m [tests] fix mamba integration simple inference precision issue (#37193)
[33m6ce238fe7a[m Fix test (#37213)
[33m12048990a9[m Add new dim to `num_items_in_batch` if necessary (#36967)
[33m98601cc818[m [Phi4] add multimodal chat template (#36996)
[33mc9302c0983[m Fix static cache export (#37229)
[33m2056287940[m Updated model card for Qwen2  (#37192)
[33m3e96a0c32b[m Update falcon model card (#37184)
[33m199d7adf10[m Updated the model card for CLIP (#37040)
[33m126abe3461[m More ReDOS fixes! (#36964)
[33m3d133cc557[m Stop DOSing the Hub in the CI (#37209)
[33me90d55ebcc[m [Tests] add `min_new_tokens` to prevent flaky length checks (#37175)
[33mcbfa14823b[m No more dtype_byte_size() (#37144)
[33m7613cf1a45[m Add py.typed (#37022)
[33m32c12aaec3[m [3/N] Use pyupgrade --py39-plus to improve code (#36936)
[33m764ab0d46a[m Merge tensor operations with device transfer operations (#37097)
[33mc94c6ed397[m Fix some code annotation typos. (#37102)
[33me94d607c8b[m fix: Add 'image-text-to-text' to `TASK_MAPPING` (#37107)
[33madfc91cd46[m Try to avoid/reduce some remaining CI job failures (#37202)
[33m6f5dc9c82e[m Fixes DynamicCache export issues due to control flow and inplace modifications (#36652)
[33ma165458901[m Add device workaround for int4 weight only quantization after API update (#36980)
[33med95493ce0[m Skip code `307` in `RequestCounter` (#36953)
[33m211e4dc9a4[m [chat-template] fix video loading (#37146)
[33m800510c67b[m [doc] Fix link for Quark quantization page (#37179)
[33m41f5c3216c[m Revert #37031 (#37178)
[33mbc2dea3f54[m Fix meta state dict loading with quantizers (#37136)
[33m35253076f4[m Avoid pipeline test failing related to Hub call (#37170)
[33mbf41e54fc8[m Fixes the inconsistency of the optionality of attention_mask (#37153)
[33m3249c5dc15[m Refactor attention for SigLIP based models (#36981)
[33m24e311f42b[m fix XPU UT error case brough by RNG difference btw XPU and CUDA (#37121)
[33m897ff9af0e[m [`ModernBERT`] Never save 'reference_compile' config; should be set based on end user (#36305)
[33mc0bd8048a5[m Make canine model exportable by removing unncessary complicated logic (#37124)
[33m60b75d99b6[m Only count num items in batch when needed (#36867)
[33mfac70ff3c0[m Convert `_VALID_DICT_FIELDS` to class attribute for shared dict parsing in subclasses (#36736)
[33mae34bd75fd[m Use public export API on torch 2.5 and future (#36781)
[33m8f6b27eb5c[m enable `test_assisted_decoding_in_different_gpu` test on XPU (#37120)
[33m737cbd2109[m Fix llava xpu tests. (#37130)
[33m3a6ab46a0b[m add gpt2 test on XPU (#37028)
[33m4b13a02920[m Fix std initialization in Idefics variants (#37100)
[33m786d9c5ed9[m Fix more inefficient PT operations (#37060)
[33ma1e389e637[m Refactor `return_dict` logic to remove complicated if/else paths (#36794)
[33mf304318f5f[m Remove low_cpu_mem_usage and _fast_init (#36963)
[33m8805600406[m [qwen3] fix generation tests (#37142)
[33me686fed635[m [Feature] Support using FlashAttention2 on Ascend NPU (#36696)
[33ma03cee7a1d[m skip (#37141)
[33m3b07ca78bb[m Export T5 (encoder-decoder) to ExecuTorch (#36486)
[33m475664e2c6[m [tests] remove cuda-only test marker in `AwqConfigTest`  (#37032)
[33m0710e9b1e8[m Create and Expose SamVisionModel as public for better accessibility (#36493)
[33mf99c279d20[m Remove deprecated code (#37059)
[33md1efaf0318[m RWKV: fix mask warning typo (#37114)
[33m19919689b2[m Fix Gemma3 embedding scaling (#37109)
[33md0b65bb479[m [MLU] Fix FA2 check error, remove deepspeed-mlu deps. (#36159)
[33mad63d20dff[m fix whisper re-compile (#36712)
[33m286393fbb1[m enable tp on CPU (#36299)
[33m4705b04c74[m Fix 4090/ada not detected as having FP8 support (#37067)
[33m2b4734bd49[m Support passing flash_attn_kwargs when gradient_checkpointing is enabled (#37037)
[33mbd41b9c1ac[m Gaudi: Fix the pipeline failed issue with hpu device (#36990)
[33m6acd5aecb3[m Adding Qwen3 and Qwen3MoE (#36878)
[33m0d6a60fe55[m 🌐 [i18n-KO] Translated `qwen2_vl.md` to Korean (#36750)
[33mb7fc2daf8b[m Kenlm (#37091)
[33mbab605dd04[m [Cache] rename dtype attribute 🚨 🚨  (#37044)
[33m9fd9476005[m [generate] beam search -- fix output cropping (#37080)
[33m257bc670fb[m fixed typo. (#37057)
[33m2bea6bf24e[m Fix AttentionInterface following feedback (#37010)
[33ma86dad56bc[m Fix state_dict map location when quantized (#37086)
[33md6064754ea[m Update w/ new account (#37084)
[33m581cf96e0c[m fix tied weigths issue  (#37031)
[33meca74d1367[m [WIP] add deepseek-v3 (#35926)
[33m52cc204dd7[m [blip-2] Fix dtype mismatch when keep in fp32  (#37068)
[33maa3778afc2[m Change deprecated PT functions (#37041)
[33mc90e6e9625[m Fix some typos about benchmark scripts. (#37027)
[33m1fcaad6df9[m Use `lru_cache` for tokenization tests (#36818)
[33m3af425d4c6[m fix: AttributeError: 'LlavaProcessor' object has no attribute 'image_token_id' (#37026)
[33m064cd7cdac[m Fix SDPA implementation in Qwen2-VL (issues with torch==2.6.0) (#36891)
[33m348f3285c5[m fix: Fully remove legacy cache from Llama (#36958)
[33md6b3c7486b[m fixed typo (#37036)
[33m6cc9c8d7d1[m Remove deprecated batch_size parameter (#37007)
[33m4cc65e990f[m Replace default split function with jnp.split() in flax models (#37001)
[33m41a0e58e5b[m Set weights_only in torch.load (#36991)
[33mde77f5b1ec[m Fix typing for None valued variables (#37004)
[33m8c5e29bad5[m Avoid unnecessary device operations in loss computing (#36950)
[33m471cf1de63[m clean pipeline question_answering. (#36986)
[33m29f322d04d[m [generate, cache] handle more complex device maps (#37014)
[33mfb8e6c50e4[m [audio utils] fix fft_bin_width computation (#36603)
[33me97c760006[m [chat templates} support loading audio from video (#36955)
[33mc7bc79bd2a[m Fixup for distill_any_depth conversion script (#37043)
[33md1eafe8d4e[m Optimize `to_py_obj` for python-native numeric lists and scalars (#36885)
[33m0e56fb69a2[m fix pegasus init weights and other copied models (#36844)
[33m7e813f9cf0[m Add Distill Any Depth (#36614)
[33m92429057d9[m Skip FP8 linear tests For device capability < 9.0(#37008)
[33m279c2e302a[m remove redundant code in trainer (#36994)
[33md13c390d01[m Mark 2 tests as flaky for now (#37038)
[33md6d930a64b[m [Modeling] Load FP8 safetensors such as DeepSeek (#36828)
[33m927ce1d39f[m Fix PixtralProcessor patch_size when spatial_merge_size is used (#37019)
[33m49b5ab6a27[m Support QuestionAnswering Module for ModernBert based models. (#35566)
[33m5b08db8844[m fix transformers_cli import relative path issue (#36989)
[33m3a8ec8c467[m [docs] Attention mask image (#36970)
[33m2b550c47b2[m Remove deprecated training arguments (#36946)
[33m44715225e3[m fix typos in the code comments and error messages (#36993)
[33m79d6f9fd70[m Log the correct learning rate (#36973)
[33m13d36e89fe[m Fix device_map check for ggml files (#37003)
[33m021006e1b0[m Fix removing "cpu" from frozenset in bitsandbytes.py to allow better ROCm support. (#36975)
[33m788e1092e9[m Allow easy registration of custom attention functions (#36889)
[33mad5d40de9c[m Fix get_device_properties (#36997)
[33m8084b26294[m Fix Optional type annotation (#36841)
[33mb56d8f07e4[m Install `networkx==3.2.1` manually in some CircleCI jobs after #36957 (#37000)
[33m78afa1c537[m Use torch.expm1 (#36995)
[33m181d453069[m byebye CircleCI TF jobs (#36998)
[33me7139d06f5[m Fix tensor dtype mismatch (#36985)
[33mbe37d34f44[m 🚨Deprecate legacy argument for image-text-to-text models and adopt new behavior by default (#36307)
[33mab4656f6b7[m update bot comment again (#36974)
[33mba531278ca[m Add ruff target-version (#36971)
[33ma844297088[m [docs] Fix image link (#36869)
[33md68a91aebf[m Remove extra tensor clone in PyTorch code (#36748)
[33m121830ab47[m update examples after ruff being updated (#36972)
[33ma41677a68b[m Updated docker files to use `uv` for installing packages (#36957)
[33m3dce98a437[m typo fixed in README_fr.md (#36951)
[33mebd2029483[m Change GPUS to GPUs (#36945)
[33m69632aadb7[m Update after #36962 (#36965)
[33mc6814b4ee8[m Update ruff to `0.11.2` (#36962)
[33mbc1c90a755[m [Utils] torch version checks optionally accept dev versions (#36847)
[33m80b4c5dcc9[m Fix cuda index issue in cache allocator (#36937)
[33m0f733110a6[m Support `return_tensors` in audio chat templates (#34601)
[33m19085c28da[m fix typos in the tests directory (#36932)
[33m69bcb86c58[m Export for Phi4-mini (#36780)
[33mbe2c0e7bff[m Fixing _pre_quantization_dtype when torch_dtype is None (#36930)
[33m4303d88c09[m Add Phi4 multimodal (#36939)
[33m47e5432805[m Deprecate #36741 and map Causal to Conditional (#36917)
[33m2b8a15cc3f[m Disallow Offload to disk for gguf files (#36933)
[33m91455c1825[m Fix processor kwargs qwen2 vl (#36890)
[33m48385aa4f4[m Added support for seed in `DataCollatorForWholeWordMask` (#36903)
[33m5932606d8e[m More precise comment (#36935)
[33m2be2984462[m Fix pytorch defomr attn path (#36923)
[33m00d077267a[m [2/N] Use pyupgrade --py39-plus to improve code (#36857)
[33ma6ecb54159[m Update `trainer_pt_utils.py` docstrings for consistency (#36912)
[33mcbf924b76c[m Fix typos (#36910)
[33m340500b1a9[m Use another repo. for Mistral3 processor testing (#36925)
[33m9e125d9a2e[m Fix Compressed tensors to_dict_diff (#36922)
[33m57f551c78d[m [chameleon] fix num image token check (#36918)
[33ma41e08aa19[m tests: fix asyncio.wait() usage for python>=3.11 (#36898)
[33me28be7a692[m [Fix] Add `original_max_position_embeddings` to YARN rope_scaling optional keys (#36877)
[33m48da44be24[m Fix torch version guard at import (#36907)
[33mfe4ca2f4a7[m fix Gemma3 Config (#36893)
[33mc9d1e5238a[m Update installation.md (#36826)
[33md253de6d58[m [docs] Model docs (#36469)
[33mbeb9b5b022[m Fix Pan and Scan on batched images Gemma3 (#36864)
[33mdd3933dd65[m Simplify keep_in_fp32_modules logic (#36722)
[33m90e2df5d55[m fix: loss computation after embeddings resize - mllama (#36840)
[33m4542b8fb27[m push v4.51.0.dev0
[33m523f6e743c[m Fix: dtype cannot be str (#36262)
[33m3f9ff19b4e[m Minor Gemma 3 fixes  (#36884)
[33mf94b0c59f2[m Use `deformable_detr` kernel from the Hub (#36853)
[33m2638d54e78[m Gemma 3 tests expect greedy decoding (#36882)
[33mb8aadc31d5[m :red_circle: :red_circle: :red_circle: supersede paligemma forward to shift pos id indexing (#36859)
[33m6321876b5b[m add eustlb as an actor
[33m94f487626a[m [generate] model defaults being inherited only happens for newer models (#36881)
[33mf19d018bff[m Revert "Update deprecated Jax calls (#35919)" (#36880)
[33m62116c967f[m Make ViTPooler configurable (#36517)
[33m26c83490d2[m chore: fix typos in the tests directory (#36813)
[33m0adbc873d0[m Remove call to `.item` in `get_batch_samples` (#36861)
[33m6bb8565f0c[m FIX FSDP plugin update for QLoRA (#36720)
[33m949cca4061[m [CI] doc builder without custom image (#36862)
[33m97d2f9d8ae[m Mllama: raise better error (#35934)
[33m6a2627918d[m Refactor Aya Vision with modular (#36688)
[33m9e771bf402[m Add support for seed in `DataCollatorForLanguageModeling` (#36497)
[33mecd60d01c3[m [CI] fix update metadata job (#36850)
[33m42c489f2ae[m Gemma3: fix test (#36820)
[33m068b663f90[m [torchao] revert to get_apply_tensor_subclass (#36849)
[33m1d3f35f30a[m Add model visual debugger (#36798)
[33m6515c25953[m Add Prompt Depth Anything Model (#35401)
[33m66291778dd[m Refactor Attention implementation for ViT-based models (#36545)
[33m730d2a52e7[m DeepSpeed tensor parallel+ZeRO (#36825)
[33m1a374799ce[m Support loading Quark quantized models in Transformers (#36372)
[33mce091b1bda[m Use pyupgrade --py39-plus to improve code (#36843)
[33m3e8f0fbf44[m Fix hqq skipped modules and dynamic quant (#36821)
[33m055afdb6bb[m Fix ONNX export for sequence classification head  (#36332)
[33m487dab1b2b[m Shieldgemma2 (#36678)
[33ma63e92e2f0[m Fix: remove the redundant snippet of _whole_word_mask (#36759)
[33m8124a234ca[m Gemma 3: Adding explicit GenerationConfig and refactoring conversion … (#36833)
[33mcf8091c017[m Fix import for torch 2.0, 2.1 - guard typehint for "device_mesh"  (#36768)
[33m388e6659bf[m Update min safetensors bis (#36823)
[33mb47d9b2f8a[m [generate] clarify docstrings: when to inherit `GenerationMixin` (#36605)
[33m8e97b44087[m [modular] Sort modular skips (#36304)
[33m63380b77d4[m Pass state dict (#35234)
[33m957b05b413[m [qwen2 audio] remove redundant code and update docs (#36282)
[33mf0d5b2ff04[m Update deprecated Jax calls (#35919)
[33m1ddb64937c[m Fix fp16 ONNX export for RT-DETR and RT-DETRv2 (#36460)
[33me7337ee7be[m Pass num_items_in_batch directly to loss computation (#36753)
[33m8b479e39bb[m Saving `Trainer.collator.tokenizer` in when `Trainer.processing_class` is `None` (#36552)
[33m3f03c379d2[m fix tiktoken convert to pass AddedToken to Tokenizer (#36566)
[33m8f64b177f6[m [ForCausalLMLoss] allow users to pass shifted labels (#36607)
[33m94555437e2[m Disable inductor config setter by default (#36608)
[33m8733297b41[m Fix swanlab global step (#36728)
[33mb815fae359[m Move the warning to the documentation for DataCollatorWithFlattening (#36707)
[33m9be4728af8[m Just import torch AdamW instead (#36177)
[33m51bd0ceb9e[m Update configuration_qwen2.py (#36735)
[33m107fedc1e2[m quick fix fast_image_processor register error (#36716)
[33m258dd9cc69[m Add Space to Bitsandbytes doc (#36834)
[33mf39f4960f3[m Support tracable dynamicKVcache (#36311)
[33m63c3116530[m One more fix for reviewer assignment (#36829)
[33m7c233980f4[m [gemma 3] multimodal checkpoints + AutoModelForCausalLM (#36741)
[33mb11050d6a2[m enable OffloadedCache on XPU from PyTorch 2.7 (#36654)
[33me8d960329e[m Add option for ao base configs (#36526)
[33mfef8b7f8e9[m Add attention visualization tool  (#36630)
[33m0fe0bae0a8[m [Generation] remove leftover code from end-to-end compilation (#36685)
[33ma861db01e5[m Fix Device map for bitsandbytes tests (#36800)
[33mb9374a0763[m Remove `dist": "loadfile"` for `pytest` in CircleCI jobs (#36811)
[33m4fa91b1be5[m fix "Cannot copy out of meta tensor; no data!" issue for BartForConditionalGeneration model (#36572)
[33m706703bba6[m Expectations test utils (#36569)
[33m179d02ffb8[m [generate] ✨ vectorized beam search ✨ (#35802)
[33m12f2ebef63[m Support custom dosctrings in modular (#36726)
[33m00915d3041[m Fix chameleon's TypeError because inputs_embeds may None (#36673)
[33m14b597f518[m Fix casting dtype for qunatization (#36799)
[33m30580f035b[m Fix Mistral3 tests (#36797)
[33mdb1d4c5a0b[m Loading optimizations (#36742)
[33m7baf00089a[m Update SHA for `tj-actions/changed-files` (#36795)
[33m3017536ebf[m fix hqq due to recent modeling changes (#36771)
[33me959530b8f[m Add Mistral3 (#36790)
[33mbd92073692[m Fix gemma3_text tokenizer in mapping (#36793)
[33m7426d02ea8[m Fixing typo in gemma3 image_processor_fast and adding a small test (#36776)
[33m19b9d8ae13[m chore: fix typos in tests directory (#36785)
[33m7f5077e536[m fix typos in the tests directory (#36717)
[33mcbfb8d7b27[m doc: Clarify `is_decoder` usage in PretrainedConfig documentation (#36724)
[33mac1a1b66b9[m [docs] Update README (#36265)
[33mcff4caa0c1[m [CI] remove redundant checks in `test_eager_matches_sdpa_inference` (#36740)
[33me3af4fec91[m [MINOR:TYPO] Update hubert.md (#36733)
[33mc8a2b25f91[m Fix `TrainingArguments.torch_empty_cache_steps` post_init check (#36734)
[33m8e67230860[m Fix test isolation for clear_import_cache utility (#36345)
[33m27361bd218[m fix xpu tests (#36656)
[33mda7d64f4ff[m Allow ray datasets to be used with trainer (#36699)
[33m2256875a77[m fix can_generate (#36570)
[33m9e94801146[m enable/disable compile for quants methods (#36519)
[33mc53d53da89[m 🚨🚨🚨 Fix sdpa in SAM and refactor relative position embeddings (#36422)
[33mfc8764c9a6[m [Generation, Gemma 3] When passing a custom `generation_config`, overwrite default values with the model's base `generation_config` (#36684)
[33mf263e88dcf[m Update self-push-caller.yml
[33m6f3e0b68e0[m Fix grad accum arbitrary value (#36691)
[33m2c2495cc7b[m Fix post_init() code duplication (#36727)
[33m25992b493c[m 🌐 [i18n-KO] Translated codegen.md to Korean (#36698)
[33m42ebb6c23e[m [tests] Parameterized `test_eager_matches_sdpa_inference` (#36650)
[33m9215cc62d4[m Try working around the processor registration bugs (#36184)
[33m691d1b52c3[m Fix/best model checkpoint fix (#35885)
[33m3bd1a0ddf1[m [model loading] don't `gc.collect()` if only 1 shard is used (#36721)
[33m8cb522b419[m Cleanup the regex used for doc preprocessing (#36648)
[33m72861e11eb[m Make the flaky list a little more general (#36704)
[33m53742b11f5[m Gemma3 processor typo (#36710)
[33m69bc848480[m Add support for fast image processors in add-new-model-like CLI (#36313)
[33m48ef468c74[m Final CI cleanup (#36703)
[33mb070025aa6[m Add GGUF support to T5-Encoder (#36700)
[33m4a60bae8e2[m Handling an exception related to HQQ quantization in modeling (#36702)
[33m09a309d273[m fix: fsdp sharded state dict wont work for save_only_model knob (#36627)
[33m2a004f9ff1[m Add loading speed test (#36671)
[33ma3201cea14[m [CI] Automatic rerun of certain test failures (#36694)
[33md84569387f[m chore: fix typos in utils module (#36668)
[33m32c95bd847[m Fix dtype for params without tp_plan (#36681)
[33mbb965d8e87[m fix type annotation for ALL_ATTENTION_FUNCTIONS (#36690)
[33m1c287aecfc[m Change Qwen2_VL image processors to have init and call accept the same kwargs (#36207)
[33m65b8e38aac[m Upgrading torch version and cuda version in quantization docker (#36264)
[33m87b30c3589[m fix wandb hp search unable to resume from sweep_id (#35883)
[33m47cc4da351[m Changing the test model in Quanto kv cache (#36670)
[33mbc3d5781e7[m Fix slicing for 0-dim param (#36580)
[33mfbb18ce68b[m Update config.torch_dtype correctly (#36679)
[33mc4161238bd[m [Cache] Don't initialize the cache on `meta` device (#36543)
[33m79254c9b61[m Fix rescale normalize inconsistencies in fast image processors (#36388)
[33m48292a9848[m Refactor siglip2 fast image processor (#36406)
[33mea219ed164[m Remove differences between init and preprocess kwargs for fast image processors (#36186)
[33mcc3a361b46[m [quants] refactor logic for modules_to_not_convert (#36672)
[33mbc3253f076[m Remove hardcoded slow image processor class in processors supporting fast ones (#36266)
[33m0013ba61e5[m Fix Failing GPTQ tests (#36666)
[33mc7eb95581a[m Don't accidentally mutate the base_model_tp_plan (#36677)
[33m071a161d3e[m [core] Large/full refactor of `from_pretrained` (#36033)
[33m7652804d23[m Fix bnb regression due to empty state dict (#36663)
[33m994cad2790[m [CI] gemma 3 `make fix-copies` (#36664)
[33m2829013d2d[m fix block mask typing (#36661)
[33m89f6956015[m HPU support (#36424)
[33m50d3530aa0[m Gemma3 (#36658)
[33m81aa9b2e07[m fix typos in the docs directory (#36639)
[33mcb384dcd7a[m Fix gguf docs (#36601)
[33m1e4286fd59[m Remove research projects (#36645)
[33med1807bab3[m [docs] Update docs dependency (#36635)
[33mb80b3ec529[m Stop warnings from unnecessary torch.tensor() overuse (#36538)
[33m556d2c23c6[m Remove remote code warning (#36285)
[33mb1a51ea464[m Fix AriaForConditionalGeneration flex attn test (#36604)
[33md126f35427[m Proper_flex (#36643)
[33md8663cb8c5[m Fix bugs in mllama image processing (#36156)
[33m1c4b62b219[m Refactor some core stuff (#36539)
[33me9756cdbc7[m [docs] Serving LLMs (#36522)
[33maf9b2eaa54[m chore: fix typos in language models (#36586)
[33ma929c466d0[m Fix auto-assign reviewers (#36631)
[33m858545047c[m [`HybridCache`] disable automatic compilation (#36620)
[33m94ae1ba5b5[m Fix check for XPU. PyTorch >= 2.6 no longer needs ipex. (#36593)
[33ma1cf9f3390[m Fixed datatype related issues in `DataCollatorForLanguageModeling` (#36457)
[33m4fce7a0f0f[m Bump jinja2 from 3.1.5 to 3.1.6 in /examples/research_projects/decision_transformer (#36582)
[33mf2fb41948e[m Update "who to tag" / "who can review" (#36394)
[33m1b9978c360[m Update chat_extras.md  with content correction (#36599)
[33mf2e197c30a[m Github action for auto-assigning reviewers (#35846)
[33m8a16edce67[m Export base streamer. (#36500)
[33m6f775970c7[m avoid errors when the size of `input_ids` passed to `PrefixConstrainedLogitsProcessor` is zero (#36489)
[33m51ed61e2f0[m Mention UltraScale Playbook 🌌 in docs (#36589)
[33m159445d044[m fix: argument (#36558)
[33m5275ef6f3d[m [XGLM] tag tests as slow (#36592)
[33mc1b24c0b73[m [bark] fix loading of generation config (#36587)
[33m0440dbc0e1[m Integrate SwanLab for offline/online experiment tracking and local visualization (#36433)
[33mbc30dd1efb[m Modular Conversion --fix_and_overwrite on Windows (#36583)
[33m9e385109cf[m Delete redundancy if case in model_utils (#36559)
[33macc49e390d[m Bump transformers from 4.38.0 to 4.48.0 in /examples/research_projects/pplm (#36540)
[33m9e84b38135[m chore: enhance message descriptions in parameters,comments,logs and docstrings (#36554)
[33m6966fa1901[m Fix typos . (#36551)
[33m996f512d52[m Fix typos in tests (#36547)
[33m752ef3fd4e[m guard torch version for uint16 (#36520)
[33m66f29aaaf5[m chore: enhance messages in docstrings (#36525)
[33m89d27fa6ff[m Fix links in quantization doc (#36528)
[33mc0c5acff07[m Fix bamba tests amd (#36535)
[33m37508816d6[m chore: Fix typos in docs and examples (#36524)
[33m84f0186e89[m Add aya (#36521)
[33mc0f8d055ce[m [docs] Redesign (#31757)
[33m6aa9888463[m Remove unused code (#36459)
[33m9fe82793ee[m [Style] fix E721 warnings (#36474)
[33m1975be4d97[m Fix edge case for continue_final_message (#36404)
[33m2aff938992[m Fix pipeline+peft interaction (#36480)
[33m28159aee63[m chore: fix message descriptions in arguments and comments (#36504)
[33macb8586dd9[m Fix some typos in docs (#36502)
[33m0463901c92[m fix torch_dtype, contiguous, and load_state_dict regression (#36512)
[33m3e83ee75ec[m Fix kwargs UserWarning in SamImageProcessor (#36479)
[33m9e3a1072c2[m Check `TRUST_REMOTE_CODE` for `RealmRetriever` for security (#36511)
[33m4d8259d245[m Fix loading zero3 weights (#36455)
[33mdcbdf7e962[m Fix _load_state_dict_into_meta_model with device_map=None (#36488)
[33ma40f1ac602[m Fix couples of issues from #36335 (#36453)
[33m2c5d038f92[m Add Got-OCR 2 Fast image processor and refactor slow one (#36185)
[33m51083d1bac[m [docs] fix bug in deepspeed config (#36081)
[33m02776d2c6a[m Fix loading models with mismatched sizes (#36463)
[33m222505c7e4[m [GroundingDino] Fix grounding dino loss 🚨 (#31828)
[33m482d17be60[m Fix `hub_retry` (#36449)
[33m6a876462c3[m Lazy import libraries in `src/transformers/image_utils.py` (#36435)
[33m8aed019764[m [generate] `torch.distributed`-compatible `DynamicCache` (#36373)
[33m17792556b2[m [save_pretrained ] Skip collecting duplicated weight (#36409)
[33m2d6cc0dfde[m Add `contents: write` (#36445)
[33m549db241e5[m Fix another permission (#36444)
[33ma8e4fe45fd[m Fix permission (#36443)
[33md0727d92cd[m Change PR to draft when it is (re)opened (#36417)
[33m8ede897c30[m restrict cache allocator to non quantized model (#36428)
[33ma7fbab33ae[m Fix Expected output for compressed-tensors tests (#36425)
[33m1603018e7a[m Update form pretrained to make TP a first class citizen (#36335)
[33m981c276a02[m Fix compressed tensors config (#36421)
[33md18d9c3205[m Universal Speculative Decoding `CandidateGenerator` (#35029)
[33m082834dd79[m fix: prevent model access error during Optuna hyperparameter tuning (#36395)
[33m6513e5e402[m add recommendations for NPU using flash_attn (#36383)
[33mb4965cecc5[m Fixing the docs corresponding to the breaking change in torch 2.6. (#36420)
[33m9a217fc327[m Deprecate transformers.agents (#36415)
[33m41925e4213[m Add retry hf hub decorator (#35213)
[33m9ebfda3263[m Fixed VitDet for non-squre Images (#35969)
[33mcbe0ea59f3[m Security fix for `benchmark.yml` (#36402)
[33m88d10517b4[m Fix convert_to_rgb for SAM ImageProcessor (#36369)
[33me1ce948908[m [CLI] add import guards (#36376)
[33mfb83befb14[m Fix pytorch integration tests for SAM (#36397)
[33mca6ebcb9bc[m chore: fix function argument descriptions (#36392)
[33m7c8916ddb5[m fix audio classification pipeline fp16 test on cuda (#36359)
[33mc3700b0eee[m [tests] enable autoawq tests on XPU  (#36327)
[33mb4b9da6d9b[m tests: revert change of torch_require_multi_gpu to be device agnostic (#35721)
[33md80d52b007[m addressing the issue #34611 to make FlaxDinov2 compatible with any batch size (#35138)
[33m3a02fe56c2[m Added handling for length <2 of suppress_tokens for whisper (#36336)
[33mda4ab2a1b6[m Fix doc formatting in forward passes & modular (#36243)
[33m92abc0dae8[m Update _get_eval_sampler to reflect Trainer.tokenizer is deprecation  self.tokenizer -> self.processing_class (#36315)
[33m9d6abf9778[m enable torchao quantization on CPU (#36146)
[33m401543a825[m Fix `is_causal` fail with compile (#36374)
[33mbc65f3fc1c[m [modular] Do not track imports in functions (#36279)
[33m4b5cf5496d[m Load models much faster on accelerator devices!! (#36380)
[33m931e5f4ac3[m Update modeling_llava_onevision.py (#36391)
[33m2ab7bdc403[m notify new model merged to `main` (#36375)
[33m05dfed06d7[m [Modeling] Reduce runtime when loading missing keys (#36312)
[33m18276b03f7[m fix(type): padding_side type should be Optional[str] (#36326)
[33mf4684a6eb2[m Update amd pytorch index to match base image (#36347)
[33m2af272c101[m Add autoquant support for torchao quantizer (#35503)
[33m977a61f743[m Change slack channel for mi250 CI to amd-hf-ci (#36346)
[33m884a8ea1f0[m Improve model loading for compressed tensor models (#36152)
[33m4dbf17c17f[m [tests] enable bnb tests on xpu (#36233)
[33m92c5ca9dd7[m Fix exploitable regexes in Nougat and GPTSan/GPTJNeoXJapanese (#36121)
[33m547911e727[m Uses Collection in transformers.image_transforms.normalize (#36301)
[33m7c5bd24ffa[m [tests] make quanto tests device-agnostic (#36328)
[33m678885bbbd[m [CI] Check test if the `GenerationTesterMixin` inheritance is correct 🐛 🔫  (#36180)
[33ma957b7911a[m Add SigLIP 2 (#36323)
[33m14552cbd7c[m VLMs: even more clean-up (#36249)
[33me18f233f6c[m Fix default attention mask of generate in MoshiForConditionalGeneration (#36171)
[33m27d1707586[m [smolvlm] make CI green (#36306)
[33meffaef334b[m fix: prevent second save in the end of training if last step was saved already (#36219)
[33m5412ff1a13[m Fix typo in Pixtral example (#36302)
[33m4397dfcb71[m SmolVLM2 (#36126)
[33mf2ab182dca[m Ignore conversion files in test fetcher (#36251)
[33me8531a0e33[m Fix broken CI on release branch due to missing conversion files  (#36275)
[33m5e2183f344[m Make cache traceable (#35873)
[33m31bb662db1[m Fix callback handler reference (#36250)
[33m78d6484675[m docs: Update README_zh-hans.md (#36269)
[33me5cea20743[m Add Example for Custom quantization (#36286)
[33me3d99ec2f5[m [tests] make `test_from_pretrained_low_cpu_mem_usage_equal` less flaky (#36255)
[33m99adc74462[m [tests] remove flax-pt equivalence and cross tests (#36283)
[33mfa8cdccd91[m [tests] deflake dither test (#36284)
[33m60226c6ff3[m TP initialization module-by-module (#35996)
[33m0863eef248[m [tests] remove `pt_tf` equivalence tests (#36253)
[33m1a81d774b1[m Add dithering to the `Speech2TextFeatureExtractor` API. (#34638)
[33m9f51dc2535[m Add support for post-processing kwargs in image-text-to-text pipeline (#35374)
[33m9b479a245b[m Uniformize LlavaNextVideoProcessor kwargs (#35613)
[33m8ee50537fe[m Qwen2VL fix cos,sin dtypes to float when used with deepspeed (#36188)
[33m8eaae6bee9[m Added Support for Custom Quantization (#35915)
[33m07182b2e10[m GitModelIntegrationTest - flatten the expected slice tensor (#36260)
[33m4d2de5f63c[m Fix XGLM loss computation (PyTorch and TensorFlow) (#35878)
[33mc3ba53303b[m feat: add support for tensor parallel training workflow with accelerate (#34194)
[33me6cc410d5b[m Remove flakiness in VLMs  (#36242)
[33mfdcfdbfd22[m Fix TorchAoConfig not JSON serializable (#36206)
[33m626666c444[m Au revoir flaky `test_fast_is_faster_than_slow` (#36240)
[33m429f1a682d[m [tests] remove `test_export_to_onnx` (#36241)
[33mdae8708c36[m Add compressed tensor in quant dockerfile (#36239)
[33m3e970dbbf1[m Bump transformers from 4.38.0 to 4.48.0 in /examples/research_projects/codeparrot/examples (#36237)
[33m77aa9fc076[m [generate] Fix encoder decoder models attention mask (#36018)
[33m55493f1390[m [tests] remove tf/flax tests in `/generation` (#36235)
[33mc877c9fa5b[m v4.45.0-dev0
[33m7ec35bc3bd[m Add missing atol to torch.testing.assert_close where rtol is specified (#36234)
[33mdad513e0c2[m [generate] remove cache v4.47 deprecations (#36212)
[33m936aeb70ab[m AMD DeepSpeed image additional HIP dependencies (#36195)
[33m23d6095e8f[m Fix `LlavaForConditionalGenerationModelTest::test_config` after #36077 (#36230)
[33mfae0f3dde8[m [tests] fix `EsmModelIntegrationTest::test_inference_bitsandbytes`  (#36225)
[33mdd16acb8a3[m set `test_torchscript = False` for Blip2 testing  (#35972)
[33m0a9923a609[m Use `args.num_workers` in `check_modular_conversion.py` (#36200)
[33ma570e2ba87[m add shared experts for upcoming Granite 4.0 language models (#35894)
[33m7ae7e87a09[m Add @require_bitsandbytes to Aria test_batched_generation (#36192)
[33mbcfc9d795e[m [Bugfix] Fix reloading of pixtral/llava configs (#36077)
[33m0c78ef6cd3[m 🔴 VLM: compile compatibility (#35724)
[33mb45cf0e90a[m Guard against unset resolved_archive_file (#35628)
[33m96f01a36ac[m Revert qwen2 breaking changes related to attention refactor (#36162)
[33mcb586a3999[m Add require_read_token to fp8 tests (#36189)
[33m5f726f8b8e[m New HIGGS quantization interfaces, JIT kernel compilation support. (#36148)
[33m15ec971b8e[m Prepare processors for VideoLLMs (#36149)
[33m33d1d715b0[m Add ImageProcessorFast to Qwen2.5-VL processor (#36164)
[33m1931a35140[m Chat template docs (#36163)
[33m3bf02cf440[m CI: fix `test-save-trainer` (#36191)
[33m0ae93d31ce[m Add support for partial rotary embeddings in Phi3 model (#35947)
[33m336dc69d63[m Uniformize OwlViT and Owlv2 processors (#35700)
[33me6a7981711[m Fix make_batched_videos and add tests (#36143)
[33m8fd4bc7d1d[m Fix a mistake in #36175 (#36179)
[33mb1a2de075d[m Follow up to SpQR integration (#36176)
[33m12962fe84b[m Fix the key name for _load_rng_state under torch.cuda (#36138)
[33mbfe46c98b5[m Make `check_repository_consistency` run faster by MP (#36175)
[33m5f0fd1185b[m Optimize Qwen2VL vision model by precomputing cos/sin embeds before ViT blocks (#35837)
[33md72642bccc[m Use tqdm auto (#35726)
[33m62c7ea0201[m CI: avoid human error, automatically infer generative models (#33212)
[33m06231fdfc7[m add disable compile option (#36161)
[33m0ca7259217[m fix training issues (#36158)
[33m845b0a2616[m Efficient Inference Kernel for SpQR  (#34976)
[33mc5506f4f00[m Bump transformers from 4.38.0 to 4.48.0 in /examples/research_projects/adversarial (#36168)
[33md7c5d1b539[m Bump transformers from 4.38.0 to 4.48.0 in /examples/tensorflow/language-modeling-tpu (#36167)
[33m636ee57489[m [generate] revert change in Aria: the maximum cache length must match `max_length` (#36120)
[33mb41591d847[m Fix : fix doc fp8 (#36173)
[33mb079dd1fa2[m Fix red CI (#36174)
[33md114a6f78e[m [Modular] skip modular checks based on diff (#36130)
[33m6397916dd2[m Remove loading custom kernel for RT-DETRv2 (#36098)
[33mefe72fe21f[m Adding FP8 Quantization to transformers (#36026)
[33mc82319b493[m Helium documentation fixes (#36170)
[33m8f137b2427[m Move `DataCollatorForMultipleChoice` from the docs to the package (#34763)
[33m35c155052d[m Fix PretrainedTokenizerFast check => Fix PretrainedTokenizerFast Save (#35835)
[33m3c912c9089[m docs: fix return type annotation of `get_default_model_revision` (#35982)
[33m6a1ab634b6[m qwen2.5vl: fix bugs when using flash2+bf16 or num_return_sequences>1 (#36083)
[33md419862889[m Fix tests for vision models (#35654)
[33me60ae0d078[m Replace deprecated update_repo_visibility (#35970)
[33m9065cf0d92[m Fix Gemma2 dtype issue when storing weights in float16 precision (#35398)
[33m08ab1abff4[m Add reminder config to issue template and print DS version in env (#35156)
[33m950cfb0b4f[m Fix PaliGemma Pad Token Masking During Training #35855 (#35859)
[33m1614d196e8[m Mllama fsdp (#36000)
[33m847854b023[m Add git LFS to AMD docker image (#36016)
[33m9985d06add[m skip `test_initialization` for `VitPoseBackboneModelTest` for now (#36154)
[33m4a5a7b991a[m Fix test fetcher (#36129)
[33m1fae54c721[m Add more rigerous non-slow grad accum tests (#35668)
[33mf869d486d3[m Update doc re list of models supporting TP (#35864)
[33m281c0c8b5b[m adding option to save/reload scaler (#34932)
[33ma33ac830af[m Fix multi gpu loss sync condition, add doc and test (#35743)
[33m08c4959a23[m  Optim: APOLLO optimizer integration (#36062)
[33m2440512723[m multi-gpu: fix tensor device placements for various models (#35763)
[33mbefea8c4f0[m 🚨 Remove cache migration script (#35810)
[33md52a9d08ce[m Bump cryptography from 43.0.1 to 44.0.1 in /examples/research_projects/decision_transformer (#36142)
[33m31e4831b98[m Bump transformers from 4.38.0 to 4.48.0 in /examples/research_projects/vqgan-clip (#36136)
[33m243aeb7c4a[m Fix Gradient Checkpointing for Deberta & Deberta-V2 using PEFT / Adapters (#35898)
[33m8a2f062eac[m [commands] remove deprecated/inoperational commands (#35718)
[33m8fc6ecba4f[m VLM: enable skipped tests (#35746)
[33md6897b46bd[m Add utility for Reload Transformers imports cache for development workflow #35508 (#35858)
[33m1cc7ca3295[m Whisper: remove redundant assisted generation tests (#34814)
[33m0cd5e2dfd0[m added warning to Trainer when label_names is not specified for PeftModel (#32085)
[33m377d8e2b9c[m add RAdamScheduleFree optimizer (#35313)
[33mf5fff672db[m Add pipeline parallel plan to `PretrainedConfig` and `PreTrainedModel` (#36091)
[33m11afab19c0[m [docs] update awq doc (#36079)
[33m9b69986e8a[m [docs] minor doc fix (#36127)
[33m1b57de8dcf[m Make `output_dir` Optional in `TrainingArguments` #27866 (#35735)
[33m03534a92f8[m update tiktoken integ to use converted (#36135)
[33m3a5c328fd8[m Fix CI issues  (#35662)
[33m775252abd4[m Fix max size deprecated warning (#34998)
[33m5489fea557[m update awesome-transformers.md. (#36115)
[33m76048be419[m fix: typos in documentation files (#36122)
[33mf42d46ccb4[m Add common test for `torch.export` and fix some vision models (#35124)
[33m1779f5180e[m Fix nighlty CIs: missing atols (#35903)
[33m1feebb5b41[m AutoformerForPrediction test add atol (#36017)
[33mbe2ac0916a[m [generate] shape checks in tests compatible with fixed-length caches (+ some minor fixes) (#35993)
[33m9510ae39d9[m fix bnb warning (#36116)
[33m09261ccf12[m [Bugfix] fix file name of docstring in utils/check_table.py (#36108)
[33md4a6b4099b[m Revert checkpoint tmp dir (#36112)
[33m0baf003915[m Refactor OPT model (#36101)
[33m924f1c717a[m Remove Multi-threaded image conversion for fast image processors (#36105)
[33m3897f2caf8[m Enable pytest live log and show warning logs on GitHub Actions CI runs (#35912)
[33m48a309d0d2[m Support constant lr with cooldown (#35453)
[33m9a6be63fdb[m Add Apple's Depth-Pro for depth estimation (#34583)
[33mc399921965[m Paligemma: revert #36084 (#36113)
[33meebd2c972c[m Chat template: update for processor (#35953)
[33m5bd7694781[m Processors: allow tuples of images when checking (#36084)
[33m3a3b06ace4[m fix MllamaVisionAttention typehint (#35975)
[33m6b55046213[m [docs] fix not-working example code in `perf_infer_gpu_one.md` (#36087)
[33m14ca7f1452[m [docs] fix typo (#36080)
[33mc361b1e3d9[m [docs] fix model checkpoint name (#36075)
[33mba29a439ad[m Fix OS err (#36094)
[33ma18b7fdd9e[m Move audio top_k tests to the right file and add slow decorator (#36072)
[33m014047e1c8[m Fix bug in apply_rotary_pos_emb_flashatt: in Qwen2-5-VL (#36065)
[33m006d9249ec[m Adding RT-DETRv2 for object detection (#34773)
[33m6246c03260[m [docs] fix outdated example code in `trainer.md` (#36066)
[33m4563ba2c6f[m Fix StopStringCriteria to handle tokens above len(tokenizer) (#35797)
[33m28f73bc307[m Fix model kwargs (#35875)
[33m1590c66430[m Fix words typos in ggml test. (#36060)
[33m1ce0e2992e[m Nail in edge case of torch dtype being overriden permantly in the case of an error (#35845)
[33me3458af726[m Save checkpoint to temporary directory to handle partial saves during failures (#35580)
[33m3dd1de39bb[m Paligemm